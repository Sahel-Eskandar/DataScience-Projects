{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahel-Eskandar/DataScience-Projects/blob/main/10_Hyperopt_StockPrice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHdmiYkgoixT"
      },
      "source": [
        "#Hyperparameter Tuning for Stock Price Prediction**\n",
        "\n",
        "Sahel Eskandar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwCajm72pGHc"
      },
      "source": [
        "## Import libraries and install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPrPKlucFK6q"
      },
      "source": [
        "try:\n",
        "    from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
        "except:\n",
        "    !pip install hyperopt\n",
        "    from hyperopt import Trials, STATUS_OK, tpe\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm_notebook as tqdm\n",
        "import pickle\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "import keras\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras import optimizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import CSVLogger\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "import logging\n",
        "import itertools as it\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqLMNRXIGBWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa649a11-9747-48e8-c1f5-547a63a743df"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "#os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOZU6HZlpTg-"
      },
      "source": [
        "## Arranging Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uevntWChFbs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2237f6-d027-4c48-b9e9-a9b6df2628b3"
      },
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "#print(\"checking if GPU available\", K.tensorflow_backend._get_available_gpus())\n",
        "\n",
        "print(\"current path\", os.getcwd())\n",
        "\n",
        "INPUT_PATH = os.path.join('/content/drive/My Drive/App', \"inputs\")\n",
        "OUTPUT_PATH = os.path.join('/content/drive/My Drive/App', \"outputs\")\n",
        "LOG_PATH = OUTPUT_PATH\n",
        "LOG_FILE_NAME_PREFIX = \"mar23_stock_pred_lstm_\"\n",
        "LOG_FILE_NAME_SUFFIX = \".log\"\n",
        "log_dir= OUTPUT_PATH\n",
        "\n",
        "stime = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current path /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZLtI8nK_Vob"
      },
      "source": [
        "TIME_STEPS = 60  # 2 months"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaV3n77YQs4f"
      },
      "source": [
        "def print_time(text, stime):\n",
        "    seconds = (time.time() - stime)\n",
        "    print(text + \" \" + str(seconds // 60) + \" minutes : \" + str(np.round(seconds % 60)) + \" seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qI4KxDLQu4J"
      },
      "source": [
        "def get_readable_ctime():\n",
        "    return time.strftime(\"%d-%m-%Y %H_%M_%S\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYt2STUbFyoi"
      },
      "source": [
        "def trim_dataset(mat, batch_size):\n",
        "    \"\"\"\n",
        "    trims dataset to a size that's divisible by BATCH_SIZE\n",
        "    \"\"\"\n",
        "    no_of_rows_drop = mat.shape[0] % batch_size\n",
        "    if no_of_rows_drop > 0:\n",
        "        return mat[:-no_of_rows_drop]\n",
        "    else:\n",
        "        return mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWSyyOV7QpN2"
      },
      "source": [
        "def build_timeseries(mat, y_col_index, time_steps):\n",
        "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
        "    dim_0 = mat.shape[0] - time_steps\n",
        "    dim_1 = mat.shape[1]\n",
        "    x = np.zeros((dim_0, time_steps, dim_1))\n",
        "    y = np.zeros((x.shape[0],))\n",
        "\n",
        "    for i in tqdm(range(dim_0)):\n",
        "        x[i] = mat[i:time_steps + i]\n",
        "        y[i] = mat[time_steps + i, y_col_index]\n",
        "    print(\"length of time-series i/o {} {}\".format(x.shape, y.shape))\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cT3bx06MXTK"
      },
      "source": [
        "def init_logging():\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    log_formatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
        "    root_logger = logging.getLogger()\n",
        "\n",
        "    file_handler = logging.FileHandler(os.path.join(LOG_PATH, LOG_FILE_NAME_PREFIX + get_readable_ctime()+\".log\")) # \"{0}/{1}.log\".format(LOG_PATH, LOG_FILE_NAME_PREFIX + get_readable_ctime()))\n",
        "    file_handler.setFormatter(log_formatter)\n",
        "    root_logger.addHandler(file_handler)\n",
        "\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(log_formatter)\n",
        "    root_logger.addHandler(console_handler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEXms3aXISDV"
      },
      "source": [
        "stime = time.time()\n",
        "init_logging()\n",
        "print(str(os.listdir(INPUT_PATH)))  \n",
        "df_ge = pd.read_csv(os.path.join(INPUT_PATH,\"AAPL.csv\"), engine='python')\n",
        "print(str(df_ge.shape))\n",
        "print(str(df_ge.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IqbtC1fm9FM"
      },
      "source": [
        "train_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "db = df_ge.loc[:,train_cols].values\n",
        "df_train=db[0:3083,:]\n",
        "df_test=db[3083:3853,:]\n",
        "\n",
        "print(\"Train--Test size {} {}\".format(len(df_train), len(df_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFE5htrkm5_q"
      },
      "source": [
        "# scale the feature MinMax, build array\n",
        "mat = db\n",
        "\n",
        "print(\"Deleting unused dataframes of total size(KB) {}\"\n",
        "      .format((sys.getsizeof(df_ge) + sys.getsizeof(df_train) + sys.getsizeof(df_test)) // 1024))\n",
        "\n",
        "del df_ge\n",
        "del df_test\n",
        "del df_train\n",
        "\n",
        "csv_logger = CSVLogger(OUTPUT_PATH + 'log_' + get_readable_ctime() + '.log', append=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA0fnWXOJhNH"
      },
      "source": [
        "class LogMetrics(Callback):\n",
        "\n",
        "    def __init__(self, search_params, param, comb_no):\n",
        "        self.param = param\n",
        "        self.self_params = search_params\n",
        "        self.comb_no = comb_no\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        for i, key in enumerate(self.self_params.keys()):\n",
        "            logs[key] = self.param[key]\n",
        "        logs[\"combination_number\"] = self.comb_no"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK11c6bsQlna"
      },
      "source": [
        "def data_dummy():\n",
        "    return None, None, None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpgYeVMEXMaZ"
      },
      "source": [
        "## **Data Transform**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH0u10VIG6QW"
      },
      "source": [
        "def data(batch_size, time_steps):\n",
        "    global mat\n",
        "\n",
        "    BATCH_SIZE = batch_size\n",
        "    TIME_STEPS = time_steps\n",
        "    #x_train, x_test = train_test_split(mat, train_size=0.8, test_size=0.2, shuffle=False)\n",
        "    x_train=mat[0:3083,:]\n",
        "    x_test=mat[3083:3853,:]\n",
        "\n",
        "    # scale the train and test dataset\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    x_train = min_max_scaler.fit_transform(x_train)\n",
        "    x_test = min_max_scaler.transform(x_test)\n",
        "\n",
        "    x_train_ts, y_train_ts = build_timeseries(x_train, 3, TIME_STEPS)\n",
        "    x_test_ts, y_test_ts = build_timeseries(x_test, 3, TIME_STEPS)\n",
        "    x_train_ts = trim_dataset(x_train_ts, BATCH_SIZE)\n",
        "    y_train_ts = trim_dataset(y_train_ts, BATCH_SIZE)\n",
        "    x_test_ts = trim_dataset(x_test_ts, BATCH_SIZE)\n",
        "    y_test_ts = trim_dataset(y_test_ts, BATCH_SIZE)\n",
        "    print(\"Test size(trimmed) {}, {}\".format(x_test_ts.shape, y_test_ts.shape))\n",
        "\n",
        "    print(\"Are any NaNs present in train/test matrices?{0},{1}\".format(str(np.isnan(x_train).any()),\n",
        "                                                                       str(np.isnan(x_test).any())))\n",
        "    return x_train_ts, y_train_ts, x_test_ts, y_test_ts, min_max_scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQxB_qP-XUPS"
      },
      "source": [
        "## **Search Space**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoz3rHLjQhSC"
      },
      "source": [
        "search_space = {\n",
        "    'batch_size': hp.choice('bs', [30,40,50,60,70]),\n",
        "    'time_steps': hp.choice('ts', [60]),\n",
        "    'lstm1_nodes':hp.choice('units_lsmt1', [70,80,100,130]),\n",
        "    'lstm1_dropouts':hp.uniform('dos_lstm1',0,1),\n",
        "    'lstm_layers': hp.choice('num_layers_lstm',[\n",
        "        #{\n",
        "        #    'layers':'one', \n",
        "        #},\n",
        "        #{\n",
        "        #    'layers':'two',\n",
        "        #    'lstm2_nodes':hp.choice('units_lstm2', [20,30,40,50]),\n",
        "        #    'lstm2_dropouts':hp.uniform('dos_lstm2',0,1)  \n",
        "        #},\n",
        "        {\n",
        "            'layers':'three',\n",
        "            'lstm3_nodes':hp.choice('units_lstm3', [20,30,40,50]),\n",
        "            'lstm3_dropouts':hp.uniform('dos_lstm3',0,1)  \n",
        "        }\n",
        "        ]),\n",
        "    'dense_layers': hp.choice('num_layers_dense',[\n",
        "        {\n",
        "            'layers':'one'\n",
        "        }\n",
        "        ]),\n",
        "    \"lr\": hp.uniform('lr',0,1),\n",
        "    \"epochs\": hp.choice('epochs', [30, 40, 50, 60]),\n",
        "    \"optimizer\": hp.choice('optmz',[\"sgd\", \"rms\"])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJazNDJnXfHD"
      },
      "source": [
        "## **Create Model hypopt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgD-ga9tJoDO"
      },
      "source": [
        "def create_model_hypopt(params):\n",
        "    print(\"Trying params:\",params)\n",
        "    batch_size = params[\"batch_size\"]\n",
        "    time_steps = params[\"time_steps\"]\n",
        "    x_train_ts, y_train_ts, x_test_ts, y_test_ts , min_max_scaler= data(batch_size, time_steps)\n",
        "    lstm_model = Sequential()\n",
        "    # (batch_size, timesteps, data_dim)\n",
        "    lstm_model.add(LSTM(params[\"lstm1_nodes\"], \n",
        "                        batch_input_shape=(batch_size, time_steps, x_train_ts.shape[2]), \n",
        "                        dropout=params[\"lstm1_dropouts\"],\n",
        "                        recurrent_dropout=params[\"lstm1_dropouts\"], \n",
        "                        stateful=True, return_sequences=True,\n",
        "                        kernel_initializer='random_uniform'))  \n",
        "    # ,return_sequences=True #LSTM params => dropout=0.2, recurrent_dropout=0.2\n",
        "    if params[\"lstm_layers\"][\"layers\"] == \"two\":\n",
        "        lstm_model.add(LSTM(params[\"lstm_layers\"][\"lstm2_nodes\"], \n",
        "                            dropout=params[\"lstm_layers\"][\"lstm2_dropouts\"], \n",
        "                            stateful=True))\n",
        "    elif params[\"lstm_layers\"][\"layers\"] == \"three\":\n",
        "        lstm_model.add(LSTM(params[\"lstm_layers\"][\"lstm3_nodes\"], \n",
        "                            dropout=params[\"lstm_layers\"][\"lstm3_dropouts\"], \n",
        "                            return_sequences=True, stateful=True))\n",
        "        lstm_model.add(LSTM(params[\"lstm_layers\"][\"lstm3_nodes\"], \n",
        "                            dropout=params[\"lstm_layers\"][\"lstm3_dropouts\"], \n",
        "                            stateful=True))\n",
        "    else:\n",
        "        lstm_model.add(Flatten())\n",
        "\n",
        "    if params[\"dense_layers\"][\"layers\"] == 'two':\n",
        "        lstm_model.add(Dense(params[\"dense_layers\"][\"dense2_nodes\"], activation='relu'))\n",
        "    \n",
        "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    lr = params[\"lr\"]\n",
        "    epochs = params[\"epochs\"]\n",
        "    if params[\"optimizer\"] == 'rms':\n",
        "        optimizer = optimizers.RMSprop(lr=lr)\n",
        "    else:\n",
        "        optimizer = optimizers.SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)  # binary_crossentropy\n",
        "    history = lstm_model.fit(x_train_ts, y_train_ts, epochs=epochs, verbose=2, batch_size=batch_size,\n",
        "                             validation_data=[x_test_ts, y_test_ts],\n",
        "                             callbacks=[LogMetrics(search_space, params, -1), csv_logger])\n",
        "    # for key in history.history.keys():\n",
        "    #     print(key, \"--\",history.history[key])\n",
        "    # get the highest validation accuracy of the training epochs\n",
        "    val_error = np.amin(history.history['val_loss']) \n",
        "    print('Best validation error of epoch:', val_error)\n",
        "    return {'loss': val_error, 'status': STATUS_OK, 'model': lstm_model} # if accuracy use '-' sign"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13R75baWeRaH"
      },
      "source": [
        "trials = Trials()\n",
        "best = fmin(create_model_hypopt,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=10,\n",
        "    trials=trials)\n",
        "\n",
        "pickle.dump(best, open(os.path.join(OUTPUT_PATH,\"hyperopt_res\"),\"wb\"))\n",
        "\n",
        "print(best)\n",
        "print_time(\"program completed in\", stime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI2o77t_QZt-"
      },
      "source": [
        "best_params={'batch_size': 40, \n",
        "            'dense_layers': {'layers': 'one'}, \n",
        "            'epochs': 20, \n",
        "            'lr': 0.29570537451402457, \n",
        "            'lstm1_dropouts': 0.47045134438682024, \n",
        "            'lstm1_nodes': 80, \n",
        "            'lstm_layers': {'layers': 'three', \n",
        "                            'lstm3_dropouts': 0.38607419457631276, \n",
        "                            'lstm3_nodes': 30}, \n",
        "            'optimizer': 'sgd', \n",
        "            'time_steps': 60}\n",
        "\n",
        "best=create_model_hypopt(best_params)\n",
        "\n",
        "pickle.dump(best, open(os.path.join(OUTPUT_PATH,\"hyperopt_res\"),\"wb\"))\n",
        "\n",
        "print(best)\n",
        "print_time(\"program completed in\", stime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPWacZXx3vwa"
      },
      "source": [
        "batch_size= 40 \n",
        "epochs= 10\n",
        "lr= 0.29570537451402457\n",
        "lstm1_dropouts= 0.47045134438682024\n",
        "lstm1_nodes= 80\n",
        "lstm2_dropouts= 0.38607419457631276\n",
        "lstm2_nodes= 30\n",
        "lstm3_dropouts= 0.38607419457631276\n",
        "lstm3_nodes= 30\n",
        "optimizer= 'sgd'\n",
        "time_steps= 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxWCzdfzHMGI"
      },
      "source": [
        "x_train_ts, y_train_ts, x_test_ts, y_test_ts, min_max_scaler =data(batch_size, time_steps)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(lstm1_nodes,\n",
        "               batch_input_shape=(batch_size, time_steps, x_train_ts.shape[2]), \n",
        "               dropout=lstm1_dropouts,\n",
        "               recurrent_dropout=lstm1_dropouts, \n",
        "               stateful=True, \n",
        "               return_sequences=True,\n",
        "               kernel_initializer='random_uniform'))  \n",
        "model.add(LSTM(lstm2_nodes, \n",
        "               dropout=lstm2_dropouts, \n",
        "               return_sequences=True, \n",
        "               stateful=True))\n",
        "model.add(LSTM(lstm3_nodes, \n",
        "               dropout=lstm3_dropouts, \n",
        "               stateful=True))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = optimizers.SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)  \n",
        "\n",
        "history = model.fit(x_train_ts, y_train_ts, epochs=epochs, verbose=2, batch_size=batch_size,\n",
        "                    validation_data=[x_test_ts, y_test_ts],\n",
        "                    callbacks=[LogMetrics(search_space, params, -1), csv_logger])\n",
        "val_error = np.amin(history.history['val_loss']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knx5oMRTLTXg"
      },
      "source": [
        "model.evaluate(x_test_ts, y_test_ts, batch_size=batch_size)\n",
        "y_pred = model.predict(trim_dataset(x_test_ts, batch_size), batch_size=batch_size)\n",
        "y_pred = y_pred.flatten()\n",
        "y_test_ts = trim_dataset(y_test_ts, batch_size)\n",
        "error = mean_squared_error(y_test_ts, y_pred)\n",
        "print(\"Error is\", error, y_pred.shape, y_test_ts.shape)\n",
        "print(y_pred[0:15])\n",
        "print(y_test_ts[0:15])\n",
        "\n",
        "# convert the predicted value to range of real data\n",
        "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
        "# min_max_scaler.inverse_transform(y_pred)\n",
        "y_test_ts_org = (y_test_ts * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
        "# min_max_scaler.inverse_transform(y_test_ts)\n",
        "print(y_pred_org[0:15])\n",
        "print(y_test_ts_org[0:15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vceu3f3lqh2s"
      },
      "source": [
        "# Visualize the training data\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "#plt.show()\n",
        "plt.savefig(os.path.join(OUTPUT_PATH, 'train_vis_BS_'+str(batch_size)+\"_\"+time.ctime()+'.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEG5OZBiql5K"
      },
      "source": [
        "# load the saved best model from above\n",
        "from keras.models import load_model\n",
        "\n",
        "saved_model =model\n",
        "\n",
        "#load_model(os.path.join(OUTPUT_PATH, 'hyperopt_res')) # , \"lstm_best_7-3-19_12AM\",\n",
        "print(saved_model)\n",
        "\n",
        "y_pred = saved_model.predict(trim_dataset(x_test_ts, batch_size), batch_size=batch_size)\n",
        "y_pred = y_pred.flatten()\n",
        "y_test_ts = trim_dataset(y_test_ts, batch_size)\n",
        "error = mean_squared_error(y_test_ts, y_pred)\n",
        "print(\"Error is\", error, y_pred.shape, y_test_ts.shape)\n",
        "print(y_pred[0:15])\n",
        "print(y_test_ts[0:15])\n",
        "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
        "y_test_ts_org = (y_test_ts * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_ts)\n",
        "print(y_pred_org[0:15])\n",
        "print(y_test_ts_org[0:15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S9xGvdpqnMu"
      },
      "source": [
        "# Visualize the prediction\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(y_pred_org)\n",
        "plt.plot(y_test_ts_org)\n",
        "plt.title('Prediction vs Real Stock Price')\n",
        "plt.ylabel('Price')\n",
        "plt.xlabel('Days')\n",
        "plt.legend(['Prediction', 'Real'], loc='upper left')\n",
        "#plt.show()\n",
        "plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS'+str(batch_size)+\"_\"+time.ctime()+'.png'))\n",
        "print_time(\"program completed \", stime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms-jPO6xaazU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}