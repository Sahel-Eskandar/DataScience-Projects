{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahel-Eskandar/DataScience-Projects/blob/main/16_nlp_xgboost_bert_glove_rnns_vectorizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziJQAjH1bTzf"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook will focus on text-classification and sentiment analysis. We will go through all major NLP and dat analysis techniques, some of which include:\n",
        "\n",
        "* LSTMs\n",
        "* Transformers (such as BERT)\n",
        "* Naive Bayes\n",
        "* XGBoost \n",
        "\n",
        "and much more...\n",
        "\n",
        "The first half of the notebook is focused on cleaning and pre-processing the data, while the second half builds and compares different models with the techniques mentioned above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPWzrwbYbVky"
      },
      "source": [
        "Data Source: \n",
        "\n",
        "kaggle kernels output namansood/nlp-xgboost-bert-glove-rnns-vectorizers -p /path/to/dest\n",
        "\n",
        "https://www.kaggle.com/code/namansood/nlp-xgboost-bert-glove-rnns-vectorizers/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:24.613599Z",
          "iopub.status.busy": "2021-07-22T07:19:24.613182Z",
          "iopub.status.idle": "2021-07-22T07:19:24.646832Z",
          "shell.execute_reply": "2021-07-22T07:19:24.645963Z",
          "shell.execute_reply.started": "2021-07-22T07:19:24.613517Z"
        },
        "id": "2VK8ZjqHbTzj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:24.648641Z",
          "iopub.status.busy": "2021-07-22T07:19:24.648309Z",
          "iopub.status.idle": "2021-07-22T07:19:24.654289Z",
          "shell.execute_reply": "2021-07-22T07:19:24.651771Z",
          "shell.execute_reply.started": "2021-07-22T07:19:24.648607Z"
        },
        "id": "tGHolVNhbTzk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:24.656239Z",
          "iopub.status.busy": "2021-07-22T07:19:24.655947Z",
          "iopub.status.idle": "2021-07-22T07:19:33.863838Z",
          "shell.execute_reply": "2021-07-22T07:19:33.862957Z",
          "shell.execute_reply.started": "2021-07-22T07:19:24.656182Z"
        },
        "id": "ZYZQCUolbTzl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np \n",
        "import random\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import nltk\n",
        "import spacy\n",
        "import random\n",
        "from spacy.util import compounding\n",
        "from spacy.util import minibatch\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report,accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CQG1qs7wf-8r"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.initializers import Constant\n",
        "from keras.layers import LSTM, Embedding,BatchNormalization, Dense, TimeDistributed, Dropout, Bidirectional, Flatten, GlobalMaxPool1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:33.865890Z",
          "iopub.status.busy": "2021-07-22T07:19:33.865559Z",
          "iopub.status.idle": "2021-07-22T07:19:33.873330Z",
          "shell.execute_reply": "2021-07-22T07:19:33.872399Z",
          "shell.execute_reply.started": "2021-07-22T07:19:33.865856Z"
        },
        "id": "vui-pD2lbTzl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Defining the global variables for the color schemes we will incorporate\n",
        "pblue = \"#496595\"\n",
        "pb2 = \"#85a1c1\"\n",
        "pb3 = \"#3f4d63\"\n",
        "pg = \"#c6ccd8\"\n",
        "pb = \"#202022\"\n",
        "pbg = \"#f4f0ea\"\n",
        "\n",
        "pgreen = px.colors.qualitative.Plotly[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:33.877342Z",
          "iopub.status.busy": "2021-07-22T07:19:33.876870Z",
          "iopub.status.idle": "2021-07-22T07:19:33.938927Z",
          "shell.execute_reply": "2021-07-22T07:19:33.938145Z",
          "shell.execute_reply.started": "2021-07-22T07:19:33.877299Z"
        },
        "id": "4KugOXt5bTzl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:33.940444Z",
          "iopub.status.busy": "2021-07-22T07:19:33.940088Z",
          "iopub.status.idle": "2021-07-22T07:19:33.949994Z",
          "shell.execute_reply": "2021-07-22T07:19:33.949143Z",
          "shell.execute_reply.started": "2021-07-22T07:19:33.940411Z"
        },
        "id": "5lm5mQK1bTzm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:33.951881Z",
          "iopub.status.busy": "2021-07-22T07:19:33.951504Z",
          "iopub.status.idle": "2021-07-22T07:19:33.962969Z",
          "shell.execute_reply": "2021-07-22T07:19:33.961877Z",
          "shell.execute_reply.started": "2021-07-22T07:19:33.951848Z"
        },
        "id": "oUG06LinbTzm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:33.967247Z",
          "iopub.status.busy": "2021-07-22T07:19:33.966656Z",
          "iopub.status.idle": "2021-07-22T07:19:33.975369Z",
          "shell.execute_reply": "2021-07-22T07:19:33.973015Z",
          "shell.execute_reply.started": "2021-07-22T07:19:33.967197Z"
        },
        "id": "5b0xb9XXbTzm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:33.978837Z",
          "iopub.status.busy": "2021-07-22T07:19:33.978153Z",
          "iopub.status.idle": "2021-07-22T07:19:33.989076Z",
          "shell.execute_reply": "2021-07-22T07:19:33.988156Z",
          "shell.execute_reply.started": "2021-07-22T07:19:33.978798Z"
        },
        "id": "hSrHWTdkbTzn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:33.990727Z",
          "iopub.status.busy": "2021-07-22T07:19:33.990205Z",
          "iopub.status.idle": "2021-07-22T07:19:34.021552Z",
          "shell.execute_reply": "2021-07-22T07:19:34.020758Z",
          "shell.execute_reply.started": "2021-07-22T07:19:33.990691Z"
        },
        "id": "7FpcK8PIbTzn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.dropna(axis=1, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.023188Z",
          "iopub.status.busy": "2021-07-22T07:19:34.022673Z",
          "iopub.status.idle": "2021-07-22T07:19:34.033887Z",
          "shell.execute_reply": "2021-07-22T07:19:34.032919Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.023154Z"
        },
        "id": "79sOaWlWbTzn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.rename(columns={\"v1\":\"label\", \"v2\":\"text\"}, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.035948Z",
          "iopub.status.busy": "2021-07-22T07:19:34.035332Z",
          "iopub.status.idle": "2021-07-22T07:19:34.054561Z",
          "shell.execute_reply": "2021-07-22T07:19:34.053694Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.035908Z"
        },
        "id": "Kt39lBWsbTzn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Finding maximum length of text message\n",
        "\n",
        "np.max(df['text'].apply(lambda x: len(x.split())).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygp6ge--bTzo"
      },
      "source": [
        "<h2>Exploratory data analysis</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.056504Z",
          "iopub.status.busy": "2021-07-22T07:19:34.055971Z",
          "iopub.status.idle": "2021-07-22T07:19:34.067397Z",
          "shell.execute_reply": "2021-07-22T07:19:34.066274Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.056468Z"
        },
        "id": "1SHEj4pBbTzo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Checking balance of dataset\n",
        "grouped_df = df.groupby('label').count().values.flatten()\n",
        "grouped_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2LCAR8NbTzo"
      },
      "source": [
        "The 'text' property is a string and must be specified as:\n",
        "\n",
        "      - A string\n",
        "      - A number that will be converted to a string\n",
        "      - A tuple, list, or one-dimensional numpy array \n",
        "      \n",
        "The 'x' and 'y' property is an array that may be specified as a tuple,\n",
        "    list, numpy array, or pandas Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.069410Z",
          "iopub.status.busy": "2021-07-22T07:19:34.069039Z",
          "iopub.status.idle": "2021-07-22T07:19:34.150538Z",
          "shell.execute_reply": "2021-07-22T07:19:34.149794Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.069375Z"
        },
        "id": "Rsk92WdZbTzo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "        x=['ham'],\n",
        "        y=[grouped_df[0]],\n",
        "        name='Safe',\n",
        "        text=[grouped_df[0]],\n",
        "        textposition='auto',\n",
        "        marker_color=pblue\n",
        ")\n",
        "             )\n",
        "fig.add_trace(go.Bar(\n",
        "        x=['spam'],\n",
        "        y=[grouped_df[1]],\n",
        "        name='Spam',\n",
        "        text=[grouped_df[1]],\n",
        "        textposition='auto',\n",
        "        marker_color=pg\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Class distribution in the dataset')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.152050Z",
          "iopub.status.busy": "2021-07-22T07:19:34.151723Z",
          "iopub.status.idle": "2021-07-22T07:19:34.174191Z",
          "shell.execute_reply": "2021-07-22T07:19:34.173491Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.152016Z"
        },
        "id": "s_u1HbpNbTzo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Creating series with length as index\n",
        "# Sorting the series by index i.e. length\n",
        "len_df_ham = df[df['label']=='ham'].text.apply(lambda x: len(x.split())).value_counts().sort_index()\n",
        "len_df_spam = df[df['label']=='spam'].text.apply(lambda x: len(x.split())).value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.175663Z",
          "iopub.status.busy": "2021-07-22T07:19:34.175327Z",
          "iopub.status.idle": "2021-07-22T07:19:34.182580Z",
          "shell.execute_reply": "2021-07-22T07:19:34.181403Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.175631Z"
        },
        "id": "c_-oTY0hbTzo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "len_df_ham"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.184348Z",
          "iopub.status.busy": "2021-07-22T07:19:34.183911Z",
          "iopub.status.idle": "2021-07-22T07:19:34.192433Z",
          "shell.execute_reply": "2021-07-22T07:19:34.191437Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.184315Z"
        },
        "id": "4LClrcrfbTzp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "len_df_spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.194475Z",
          "iopub.status.busy": "2021-07-22T07:19:34.193971Z",
          "iopub.status.idle": "2021-07-22T07:19:34.254265Z",
          "shell.execute_reply": "2021-07-22T07:19:34.253332Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.194440Z"
        },
        "id": "fT6p-eCnbTzp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# X-axis consists of the length of the msgs\n",
        "# Y-axis consists of the frequency of those lengths\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(\n",
        "x=len_df_ham.index,\n",
        "y=len_df_ham.values,\n",
        "name='Safe',\n",
        "fill='tozeroy',\n",
        "marker_color=pblue))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "x=len_df_spam.index,\n",
        "y=len_df_spam.values,\n",
        "name='Spam',\n",
        "fill='tozeroy',\n",
        "marker_color=pg\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Frequency of SMS lengths')\n",
        "fig.update_xaxes(range=[0, 80])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LwluEfUbTzp"
      },
      "source": [
        "We can see that the safe SMS messages are much shorter than the spam messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH6lVarIbTzp"
      },
      "source": [
        "<h2>Data preprocessing</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.256051Z",
          "iopub.status.busy": "2021-07-22T07:19:34.255700Z",
          "iopub.status.idle": "2021-07-22T07:19:34.263946Z",
          "shell.execute_reply": "2021-07-22T07:19:34.263241Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.256012Z"
        },
        "id": "nIEGDD3abTzp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def cleaning(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.265639Z",
          "iopub.status.busy": "2021-07-22T07:19:34.265368Z",
          "iopub.status.idle": "2021-07-22T07:19:34.276185Z",
          "shell.execute_reply": "2021-07-22T07:19:34.275319Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.265615Z"
        },
        "id": "uMrZll7mbTzp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.277550Z",
          "iopub.status.busy": "2021-07-22T07:19:34.277159Z",
          "iopub.status.idle": "2021-07-22T07:19:34.445098Z",
          "shell.execute_reply": "2021-07-22T07:19:34.444293Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.277522Z"
        },
        "id": "pjA1Mca9bTzp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df['text'] = df['text'].apply(cleaning)\n",
        "df['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.446495Z",
          "iopub.status.busy": "2021-07-22T07:19:34.446167Z",
          "iopub.status.idle": "2021-07-22T07:19:34.632947Z",
          "shell.execute_reply": "2021-07-22T07:19:34.632166Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.446462Z"
        },
        "id": "9Q4qZD12bTzp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Removing stop words\n",
        "stop_words = stopwords.words('english')\n",
        "more = ['u', 'im', 'c']\n",
        "stop_words = stop_words + more\n",
        "\n",
        "\n",
        "def sw_rem(text):\n",
        "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(sw_rem)\n",
        "df['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mtqTbkZbTzp"
      },
      "source": [
        "<h2>Stemming and lemmatization</h2>\n",
        "\n",
        "Documents and other forms of text use different forms of the same words, such as play, playing, played. There are families of derivationally related words that have similar meanings. Our main task with stemming and lemmatization is to reduce all these derived words into the parent/family word, therefore reducing the total vocabulary while retaining information.\n",
        "\n",
        "* **Stemming** - Omits the ends of words to achieve the goal correctly, this works **most of the times** and can also remove the derivational suffix\n",
        "\n",
        "* **Lemmatization** - Working with a vocabulary and morphological analysis of wrods, removing inflectional endings only and returning the base and dictionary form of a word.\n",
        "\n",
        "As we do not require much emphasis on words, we will focus more on stemming than lemmatization,.\n",
        "\n",
        "<h3>Stemming algorithms</h3>\n",
        "\n",
        "We have multiple algorithms to achieve our stemming goals, some of them are as follows:\n",
        "\n",
        "* PorterStemmer - Fast and efficient. Strips off the end (suffix) to produce the stems. It does not follow linguistics but rather a set of 05 rules for diferent cases. \n",
        "\n",
        "* SnowballStemmer - Generate a set of rules for any language. These are useful for non-english stemming tasks.\n",
        "\n",
        "* LancasterStemmer - Iterative algorithm, uses about 120 rules, it tries to find an applicable rule by the last character of each word. The last character may be omitted or replaced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.638034Z",
          "iopub.status.busy": "2021-07-22T07:19:34.637796Z",
          "iopub.status.idle": "2021-07-22T07:19:34.644485Z",
          "shell.execute_reply": "2021-07-22T07:19:34.643533Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.638010Z"
        },
        "id": "sIVQJkvabTzq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "stems = nltk.SnowballStemmer('english')\n",
        "\n",
        "def stemming(text):\n",
        "    text = ' '.join(stems.stem(word) for word in text.split())\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:34.648895Z",
          "iopub.status.busy": "2021-07-22T07:19:34.648598Z",
          "iopub.status.idle": "2021-07-22T07:19:35.320677Z",
          "shell.execute_reply": "2021-07-22T07:19:35.319722Z",
          "shell.execute_reply.started": "2021-07-22T07:19:34.648871Z"
        },
        "id": "FMajbaLCbTzq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df['text'] = df['text'].apply(stemming)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:35.322534Z",
          "iopub.status.busy": "2021-07-22T07:19:35.322170Z",
          "iopub.status.idle": "2021-07-22T07:19:35.327910Z",
          "shell.execute_reply": "2021-07-22T07:19:35.326929Z",
          "shell.execute_reply.started": "2021-07-22T07:19:35.322496Z"
        },
        "id": "ZmOR_yEabTzq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Creating a pipeline\n",
        "\n",
        "def pipeline(text):\n",
        "    text = cleaning(text)\n",
        "    text = ' ' .join(word for word in text.split(' ') if word not in stop_words)\n",
        "    text = ' '.join(stems.stem(word) for word in text.split(' '))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:35.329895Z",
          "iopub.status.busy": "2021-07-22T07:19:35.329475Z",
          "iopub.status.idle": "2021-07-22T07:19:36.229491Z",
          "shell.execute_reply": "2021-07-22T07:19:36.228545Z",
          "shell.execute_reply.started": "2021-07-22T07:19:35.329858Z"
        },
        "id": "lTq5zuBQbTzq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df['text'] = df['text'].apply(pipeline)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:36.231268Z",
          "iopub.status.busy": "2021-07-22T07:19:36.230908Z",
          "iopub.status.idle": "2021-07-22T07:19:36.243760Z",
          "shell.execute_reply": "2021-07-22T07:19:36.242785Z",
          "shell.execute_reply.started": "2021-07-22T07:19:36.231216Z"
        },
        "id": "PLYJUYy7bTzq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Encoding the categorical target variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(df['label'])\n",
        "\n",
        "df['label_num'] = le.transform(df['label'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me7mobO4bTzq"
      },
      "source": [
        "<h2>Visualizing tokens</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:36.245684Z",
          "iopub.status.busy": "2021-07-22T07:19:36.245322Z",
          "iopub.status.idle": "2021-07-22T07:19:36.252824Z",
          "shell.execute_reply": "2021-07-22T07:19:36.251963Z",
          "shell.execute_reply.started": "2021-07-22T07:19:36.245650Z"
        },
        "id": "U0dml2-qbTzq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# This will combine all the text values for safe sms\n",
        "#' '.join(text for text in df[df['label']=='ham'].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:36.254820Z",
          "iopub.status.busy": "2021-07-22T07:19:36.254410Z",
          "iopub.status.idle": "2021-07-22T07:19:37.225740Z",
          "shell.execute_reply": "2021-07-22T07:19:37.224934Z",
          "shell.execute_reply.started": "2021-07-22T07:19:36.254787Z"
        },
        "id": "GEaBZ0Z0bTzq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Extracting the twitter word cloud mask\n",
        "twitter_mask = np.array(Image.open('/kaggle/input/masksforwordclouds/twitter_mask3.jpg'))\n",
        "\n",
        "wc = WordCloud(background_color='white', max_words=200, mask=twitter_mask)\n",
        "\n",
        "wc.generate(' '.join(text for text in df[df['label']=='ham'].text))\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.title('Top words for safe messages', fontdict={'size':22})\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:37.227141Z",
          "iopub.status.busy": "2021-07-22T07:19:37.226819Z",
          "iopub.status.idle": "2021-07-22T07:19:37.900606Z",
          "shell.execute_reply": "2021-07-22T07:19:37.899625Z",
          "shell.execute_reply.started": "2021-07-22T07:19:37.227102Z"
        },
        "id": "efHuklgJbTzq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Extracting the twitter word cloud mask\n",
        "wc = WordCloud(background_color='white', max_words=200, mask=twitter_mask)\n",
        "\n",
        "wc.generate(' '.join(text for text in df[df['label']=='spam'].text))\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.title('Top words for Spam messages', fontdict={'size':22})\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHQbjdNubTzq"
      },
      "source": [
        "<h2>Vectorization</h2>\n",
        "\n",
        "We currently have each text record in string format. We need to convert each of those records into a vector that our models can work with. We will first do this using the bag-of-words model.\n",
        "\n",
        "We will use two major approaches here\n",
        "\n",
        "* **CountVectorizer** - Working on frequency of each word in the given string.\n",
        "\n",
        "* **Term frequency-inverse document frqeuency TFIDF** - Works on frequency divided by the appearance of the given word in the total documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:37.902642Z",
          "iopub.status.busy": "2021-07-22T07:19:37.902294Z",
          "iopub.status.idle": "2021-07-22T07:19:37.908869Z",
          "shell.execute_reply": "2021-07-22T07:19:37.908108Z",
          "shell.execute_reply.started": "2021-07-22T07:19:37.902606Z"
        },
        "id": "2H_Q-onxbTzq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "x = df['text']\n",
        "y = df['label_num']\n",
        "\n",
        "len(x), len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:37.910635Z",
          "iopub.status.busy": "2021-07-22T07:19:37.910102Z",
          "iopub.status.idle": "2021-07-22T07:19:37.922807Z",
          "shell.execute_reply": "2021-07-22T07:19:37.921852Z",
          "shell.execute_reply.started": "2021-07-22T07:19:37.910600Z"
        },
        "id": "wmhFTemSbTzr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
        "print(len(x_train), len(y_train))\n",
        "print(len(x_test), len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:37.924335Z",
          "iopub.status.busy": "2021-07-22T07:19:37.923950Z",
          "iopub.status.idle": "2021-07-22T07:19:38.072712Z",
          "shell.execute_reply": "2021-07-22T07:19:38.071942Z",
          "shell.execute_reply.started": "2021-07-22T07:19:37.924300Z"
        },
        "id": "CZeSfKTDbTzr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# First working with count vectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# instantiate the vectorizer\n",
        "count = CountVectorizer()\n",
        "count.fit(x)\n",
        "\n",
        "x_train_num = count.transform(x_train)\n",
        "x_test_num = count.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGfRWGYWbTzr"
      },
      "source": [
        "The CountVectorizer model can be tuned in a variety of ways:\n",
        "\n",
        "* Stop words - Extremely common words can be omitted by the model by setting this parameter to the language corresponding to the text.\n",
        "\n",
        "* ngram_range - It pairs up words together as features. If we consider bigrams and we have a sentence \"I am happy\", we will have two features - [\"I am\", \"am happy\"]. We can define a range of ngrams, so if we have the same sentence with a range from 1 to 2, our features will be:  `[\"I\", \"am\", \"happy\", \"I am\", \"am happy\"]`. This increase is features helps to fine tune the model.\n",
        "\n",
        "* min_df, max_df - Minimum and maximum frequencies of words of n-grams that can be used as features. If either of the conditions are not met, the feature will be omitted.\n",
        "\n",
        "* max_features - Choose the most frequent words and drop everything else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:38.075626Z",
          "iopub.status.busy": "2021-07-22T07:19:38.075383Z",
          "iopub.status.idle": "2021-07-22T07:19:38.081638Z",
          "shell.execute_reply": "2021-07-22T07:19:38.080847Z",
          "shell.execute_reply.started": "2021-07-22T07:19:38.075602Z"
        },
        "id": "hpaGBv5xbTzr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Example of a tuned model\n",
        "count_tuned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:38.083507Z",
          "iopub.status.busy": "2021-07-22T07:19:38.082839Z",
          "iopub.status.idle": "2021-07-22T07:19:38.100292Z",
          "shell.execute_reply": "2021-07-22T07:19:38.099576Z",
          "shell.execute_reply.started": "2021-07-22T07:19:38.083463Z"
        },
        "id": "y2_h-DwobTzr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Working with TF-IDF now\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "# We are using transformer here\n",
        "# If we use vectorizer, we can directly use the text\n",
        "tfidf = TfidfTransformer()\n",
        "\n",
        "tfidf.fit(x_train_num)\n",
        "x_train_tfidf = tfidf.transform(x_train_num)\n",
        "\n",
        "x_train_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stMzLo3NbTzr"
      },
      "source": [
        "<h2>Working with Embeddings - GloVe</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:38.102946Z",
          "iopub.status.busy": "2021-07-22T07:19:38.102711Z",
          "iopub.status.idle": "2021-07-22T07:19:38.107161Z",
          "shell.execute_reply": "2021-07-22T07:19:38.106145Z",
          "shell.execute_reply.started": "2021-07-22T07:19:38.102924Z"
        },
        "id": "cz2aUBrVbTzr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "text = df['text']\n",
        "label = df['label_num']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:38.109163Z",
          "iopub.status.busy": "2021-07-22T07:19:38.108808Z",
          "iopub.status.idle": "2021-07-22T07:19:38.195624Z",
          "shell.execute_reply": "2021-07-22T07:19:38.194711Z",
          "shell.execute_reply.started": "2021-07-22T07:19:38.109130Z"
        },
        "id": "Ke4YuaK3bTzr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calculating the total vocabulary\n",
        "tk = Tokenizer()\n",
        "tk.fit_on_texts(text)\n",
        "\n",
        "vocab = len(tk.word_index)+1\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Rd4foTbTzr"
      },
      "source": [
        "Now we will proceed with converting the text to numerical values and also padding the vectors so each of them are of equal length. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:38.197147Z",
          "iopub.status.busy": "2021-07-22T07:19:38.196818Z",
          "iopub.status.idle": "2021-07-22T07:19:38.210155Z",
          "shell.execute_reply": "2021-07-22T07:19:38.209330Z",
          "shell.execute_reply.started": "2021-07-22T07:19:38.197113Z"
        },
        "id": "BnSQtL-WbTzr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Maximum length\n",
        "max_len = np.max(df['text'].apply(lambda x: len(x.split())).values)\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:38.212169Z",
          "iopub.status.busy": "2021-07-22T07:19:38.211588Z",
          "iopub.status.idle": "2021-07-22T07:19:38.220511Z",
          "shell.execute_reply": "2021-07-22T07:19:38.219573Z",
          "shell.execute_reply.started": "2021-07-22T07:19:38.212134Z"
        },
        "id": "gaCkx6MxbTzr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:38.222143Z",
          "iopub.status.busy": "2021-07-22T07:19:38.221743Z",
          "iopub.status.idle": "2021-07-22T07:19:38.328154Z",
          "shell.execute_reply": "2021-07-22T07:19:38.327262Z",
          "shell.execute_reply.started": "2021-07-22T07:19:38.222109Z"
        },
        "id": "5TT47dLwbTzr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def embedding(text):\n",
        "    return tk.texts_to_sequences(text)\n",
        "\n",
        "train_padded = pad_sequences(embedding(text), 80, padding='post')\n",
        "train_padded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rthEL5XbTzs"
      },
      "source": [
        "<h2>GloVe Embeddings</h2>\n",
        "\n",
        "These embeddings are based on the principle that we can derive sematic relationships between words from their co-occurence matrix. This embedding focuses on words co-occurrences over the whole corpus. \n",
        "\n",
        "They are a form of word representation that try to merge human understanding of languages into their structure. They have a learned representation in an n-dimension space, where words with similar meanings have similar embeddings. Two similar words are represented by almost similar vectors that are at a small distance in the vector space.\n",
        "\n",
        "When using a vector space, all the words are represented as vectors in a predefined N-dimension vector space. Each word is mapped to a vector and the vector values are learned in a way that resembles a neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:19:38.329674Z",
          "iopub.status.busy": "2021-07-22T07:19:38.329341Z",
          "iopub.status.idle": "2021-07-22T07:20:00.156789Z",
          "shell.execute_reply": "2021-07-22T07:20:00.152615Z",
          "shell.execute_reply.started": "2021-07-22T07:19:38.329641Z"
        },
        "id": "DN6dVnDebTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Using our helper functions for GloVe\n",
        "\n",
        "embedding_dict = dict()\n",
        "embedding_dim = 100\n",
        "\n",
        "# Each word is represented in one line in the text file\n",
        "# Format - Word val1 val2 val3......val-n for n-dimension vector space\n",
        "\n",
        "with open('../input/glove6b100dtxt/glove.6B.100d.txt') as fp:\n",
        "    for line in fp.readlines():\n",
        "        records = line.split()\n",
        "        word = records[0]\n",
        "        vector = np.asarray(records[1:], dtype='float32')\n",
        "        embedding_dict[word] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.166993Z",
          "iopub.status.busy": "2021-07-22T07:20:00.166719Z",
          "iopub.status.idle": "2021-07-22T07:20:00.188442Z",
          "shell.execute_reply": "2021-07-22T07:20:00.187286Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.166967Z"
        },
        "id": "lCbG_BxLbTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Creating a matrix for each word as index (word numerical value extracted from tokenizer\n",
        "# with N-features (corresponding to GloVe)\n",
        "# We will replace the matrix elements by the words and their embeddings\n",
        "\n",
        "# Our embeddings will also consist embeddings for padding\n",
        "embedding_matrix = np.zeros((vocab, embedding_dim))\n",
        "\n",
        "for word, index in tk.word_index.items():\n",
        "    embed_vector = embedding_dict.get(word)\n",
        "    if embed_vector is not None:\n",
        "        embedding_matrix[index] = embed_vector\n",
        "        \n",
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.190354Z",
          "iopub.status.busy": "2021-07-22T07:20:00.189958Z",
          "iopub.status.idle": "2021-07-22T07:20:00.196997Z",
          "shell.execute_reply": "2021-07-22T07:20:00.196178Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.190315Z"
        },
        "id": "jzkSV1dxbTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# We will be creating seaborn and plotly confusion matrices\n",
        "import plotly.figure_factory as ff\n",
        "x_axes = ['Safe','Spam']\n",
        "y_axes = ['Spam', 'Safe']\n",
        "\n",
        "def conf_matrix(z, x=x_axes, y=y_axes):\n",
        "    z = np.flip(z, 0)\n",
        "    # Change each element of z to string \n",
        "    # This allows them to be used as annotations\n",
        "    z_str = [[str(y) for y in x] for x in z]\n",
        "    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_str)\n",
        "    \n",
        "    fig.update_layout(title_text='Confusion matrix', xaxis=dict(title='Predicted Value'),\n",
        "                     yaxis=dict(title='Real value'))\n",
        "    \n",
        "    fig['data'][0]['showscale'] = True\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.198932Z",
          "iopub.status.busy": "2021-07-22T07:20:00.198350Z",
          "iopub.status.idle": "2021-07-22T07:20:00.210097Z",
          "shell.execute_reply": "2021-07-22T07:20:00.209122Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.198892Z"
        },
        "id": "Vrx_tFXIbTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "categories=['Safe', 'Spam']\n",
        "def seaborn_conf(y, ypred):\n",
        "    y_true = [\"Safe\", \"Spam\"]\n",
        "    y_pred = [\"Safe\", \"Spam\"]\n",
        "    cf = confusion_matrix(y, ypred)\n",
        "    df_cm = pd.DataFrame(cf, columns=np.unique(y_true), index = np.unique(y_true))\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(df_cm, annot=True, fmt='g')\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.xlabel('Predicted value')\n",
        "    plt.ylabel('Real value')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCJ0ZULybTzs"
      },
      "source": [
        "<h2>Model creation and prediction</h2>\n",
        "\n",
        "We will first start with the **naive bayes classifier** which comes from a family of simple \"probabilistic classifiers\" based on application of Bayes theroem with strong independent assumptions between features.\n",
        "\n",
        "The model is highly scalable, with number of parameters being linear with number of variables. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.211958Z",
          "iopub.status.busy": "2021-07-22T07:20:00.211386Z",
          "iopub.status.idle": "2021-07-22T07:20:00.228441Z",
          "shell.execute_reply": "2021-07-22T07:20:00.227282Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.211923Z"
        },
        "id": "SzBNsBkFbTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Train the model - CountVectorizer model\n",
        "nb.fit(x_train_num, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.230366Z",
          "iopub.status.busy": "2021-07-22T07:20:00.229928Z",
          "iopub.status.idle": "2021-07-22T07:20:00.237137Z",
          "shell.execute_reply": "2021-07-22T07:20:00.235692Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.230320Z"
        },
        "id": "FGhn_HVSbTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Class and probability predictions\n",
        "yp_class = nb.predict(x_test_num)\n",
        "yp_prob = nb.predict_proba(x_test_num)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.239531Z",
          "iopub.status.busy": "2021-07-22T07:20:00.239017Z",
          "iopub.status.idle": "2021-07-22T07:20:00.483798Z",
          "shell.execute_reply": "2021-07-22T07:20:00.483032Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.239487Z"
        },
        "id": "ClNtr8a4bTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.accuracy_score(y_test, yp_class))\n",
        "seaborn_conf(y_test, yp_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.485464Z",
          "iopub.status.busy": "2021-07-22T07:20:00.485101Z",
          "iopub.status.idle": "2021-07-22T07:20:00.495357Z",
          "shell.execute_reply": "2021-07-22T07:20:00.494425Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.485425Z"
        },
        "id": "ikZX1rnObTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "metrics.roc_auc_score(y_test, yp_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl6Fl4dnbTzs"
      },
      "source": [
        "<h2>Working with Naive Bayes + TF-IDF</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.497337Z",
          "iopub.status.busy": "2021-07-22T07:20:00.496992Z",
          "iopub.status.idle": "2021-07-22T07:20:00.504554Z",
          "shell.execute_reply": "2021-07-22T07:20:00.503574Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.497300Z"
        },
        "id": "RvYYbgVcbTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipe = Pipeline([('bow', CountVectorizer()), \n",
        "                 ('tfid', TfidfTransformer()),  \n",
        "                 ('model', MultinomialNB())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.506660Z",
          "iopub.status.busy": "2021-07-22T07:20:00.506127Z",
          "iopub.status.idle": "2021-07-22T07:20:00.828525Z",
          "shell.execute_reply": "2021-07-22T07:20:00.827655Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.506604Z"
        },
        "id": "8qADl2TTbTzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pipe.fit(x_train, y_train)\n",
        "yp_class = pipe.predict(x_test)\n",
        "print(metrics.accuracy_score(y_test, yp_class))\n",
        "seaborn_conf(y_test, yp_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XI7iV7bbTzt"
      },
      "source": [
        "<h2>XGBoost</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.830266Z",
          "iopub.status.busy": "2021-07-22T07:20:00.829836Z",
          "iopub.status.idle": "2021-07-22T07:20:00.901031Z",
          "shell.execute_reply": "2021-07-22T07:20:00.900163Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.830200Z"
        },
        "id": "uPISmiZsbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "pipe = Pipeline([\n",
        "    ('bow', CountVectorizer()), \n",
        "    ('tfid', TfidfTransformer()),  \n",
        "    ('model', xgb.XGBClassifier(\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,\n",
        "        n_estimators=90,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='auc'\n",
        "    ))\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:00.902812Z",
          "iopub.status.busy": "2021-07-22T07:20:00.902448Z",
          "iopub.status.idle": "2021-07-22T07:20:03.667218Z",
          "shell.execute_reply": "2021-07-22T07:20:03.666339Z",
          "shell.execute_reply.started": "2021-07-22T07:20:00.902774Z"
        },
        "id": "UDCumywBbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pipe.fit(x_train, y_train)\n",
        "yp_class_test = pipe.predict(x_test)\n",
        "yp_class_train = pipe.predict(x_train)\n",
        "\n",
        "print('Training accuracy score: {}'.format(metrics.accuracy_score(y_train, yp_class_train)))\n",
        "print('Testing accuracy score: {}'.format(metrics.accuracy_score(y_test, yp_class_test)))\n",
        "\n",
        "seaborn_conf(y_test, yp_class_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLVWMr-bbTzt"
      },
      "source": [
        "<h2>LSTMs and GloVE embeddings</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:03.668864Z",
          "iopub.status.busy": "2021-07-22T07:20:03.668530Z",
          "iopub.status.idle": "2021-07-22T07:20:03.674253Z",
          "shell.execute_reply": "2021-07-22T07:20:03.673368Z",
          "shell.execute_reply.started": "2021-07-22T07:20:03.668832Z"
        },
        "id": "mMroy8EwbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:03.676099Z",
          "iopub.status.busy": "2021-07-22T07:20:03.675570Z",
          "iopub.status.idle": "2021-07-22T07:20:03.687626Z",
          "shell.execute_reply": "2021-07-22T07:20:03.686636Z",
          "shell.execute_reply.started": "2021-07-22T07:20:03.676060Z"
        },
        "id": "eg7rW6vQbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train_padded, label, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:03.689612Z",
          "iopub.status.busy": "2021-07-22T07:20:03.689063Z",
          "iopub.status.idle": "2021-07-22T07:20:07.463910Z",
          "shell.execute_reply": "2021-07-22T07:20:07.462965Z",
          "shell.execute_reply.started": "2021-07-22T07:20:03.689569Z"
        },
        "id": "_zSu_ACsbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=embedding_matrix.shape[0], \n",
        "                   output_dim=embedding_matrix.shape[1],\n",
        "                   weights=[embedding_matrix],\n",
        "                   input_length=max_len\n",
        "                   )\n",
        "         )\n",
        "model.add(Bidirectional(LSTM(max_len, return_sequences=True, recurrent_dropout=0.15)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(max_len, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(max_len, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:07.465768Z",
          "iopub.status.busy": "2021-07-22T07:20:07.465379Z",
          "iopub.status.idle": "2021-07-22T07:20:07.474024Z",
          "shell.execute_reply": "2021-07-22T07:20:07.472986Z",
          "shell.execute_reply.started": "2021-07-22T07:20:07.465725Z"
        },
        "id": "2WgO2yjxbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Defining Callbacks\n",
        "# Checkpoints in case our model stops training due to some circumstance - saving progress\n",
        "checkpoints = ModelCheckpoint('ck_model.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "# Reducing the learning rate if no improvement in validation loss over 5 epochs\n",
        "# This is to train the model better\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', vactor=0.1, verbose=1, patience=5, min_lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:20:07.475896Z",
          "iopub.status.busy": "2021-07-22T07:20:07.475355Z",
          "iopub.status.idle": "2021-07-22T07:32:02.819815Z",
          "shell.execute_reply": "2021-07-22T07:32:02.819034Z",
          "shell.execute_reply.started": "2021-07-22T07:20:07.475852Z"
        },
        "id": "2yPzfX0ubTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test), verbose=1, callbacks=[reduce_lr, checkpoints])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:32:02.823021Z",
          "iopub.status.busy": "2021-07-22T07:32:02.822770Z",
          "iopub.status.idle": "2021-07-22T07:32:02.828890Z",
          "shell.execute_reply": "2021-07-22T07:32:02.828114Z",
          "shell.execute_reply.started": "2021-07-22T07:32:02.822995Z"
        },
        "id": "KanoH59zbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Plotting the results\n",
        "def learning_curve(history, arr):\n",
        "    fig, ax=plt.subplots(1, 2, figsize=(20, 5))\n",
        "    for idx in range(2):\n",
        "        ax[idx].plot(history.history[arr[idx][0]])\n",
        "        ax[idx].plot(history.history[arr[idx][1]])\n",
        "        ax[idx].legend([arr[idx][0], arr[idx][1]])\n",
        "        ax[idx].set_xlabel('Epochs')\n",
        "        ax[idx].set_ylabel('Value')\n",
        "        ax[idx].set_title(arr[idx][0]+' X '+ arr[idx][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:32:02.830463Z",
          "iopub.status.busy": "2021-07-22T07:32:02.829970Z",
          "iopub.status.idle": "2021-07-22T07:32:03.155787Z",
          "shell.execute_reply": "2021-07-22T07:32:03.155008Z",
          "shell.execute_reply.started": "2021-07-22T07:32:02.830429Z"
        },
        "id": "OCM5efEGbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "learning_curve(history, [['loss', 'val_loss'], ['accuracy', 'val_accuracy']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:32:03.157385Z",
          "iopub.status.busy": "2021-07-22T07:32:03.157039Z",
          "iopub.status.idle": "2021-07-22T07:32:04.638234Z",
          "shell.execute_reply": "2021-07-22T07:32:04.637400Z",
          "shell.execute_reply.started": "2021-07-22T07:32:03.157349Z"
        },
        "id": "D1URS30kbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "yp = model.predict(x_test)\n",
        "yp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:32:04.641533Z",
          "iopub.status.busy": "2021-07-22T07:32:04.641281Z",
          "iopub.status.idle": "2021-07-22T07:32:05.802851Z",
          "shell.execute_reply": "2021-07-22T07:32:05.802044Z",
          "shell.execute_reply.started": "2021-07-22T07:32:04.641508Z"
        },
        "id": "jBdGjLRHbTzt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "yp = (model.predict(x_test)>0.5).astype('int32')\n",
        "yp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:32:05.804401Z",
          "iopub.status.busy": "2021-07-22T07:32:05.804054Z",
          "iopub.status.idle": "2021-07-22T07:32:06.028856Z",
          "shell.execute_reply": "2021-07-22T07:32:06.028114Z",
          "shell.execute_reply.started": "2021-07-22T07:32:05.804374Z"
        },
        "id": "r4FwSEr4bTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "seaborn_conf(y_test, yp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rIqqPvcbTzu"
      },
      "source": [
        "<h2>Transformers - BERT</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLT-LVM2bTzu"
      },
      "source": [
        "BERT has revolutionized the world of NLP by providing state-of-the-art results on many NLP tasks. BERT stands for Bidirectional Encoder Representation from Transformer. It is the state-of-the-art embedding model published by Google. It has created a major breakthrough in the field of NLP by providing greater results in many NLP tasks, such as question answering, text generation, sentence classification, and many more besides. One of the major reasons for the success of BERT is that it is a context-based embedding model, unlike other popular embedding models, such as word2vec, which are context-free."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:32:06.030435Z",
          "iopub.status.busy": "2021-07-22T07:32:06.030170Z",
          "iopub.status.idle": "2021-07-22T07:32:14.226641Z",
          "shell.execute_reply": "2021-07-22T07:32:14.225666Z",
          "shell.execute_reply.started": "2021-07-22T07:32:06.030409Z"
        },
        "id": "Uf4vgWgjbTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:32:14.228703Z",
          "iopub.status.busy": "2021-07-22T07:32:14.228328Z",
          "iopub.status.idle": "2021-07-22T07:32:15.483724Z",
          "shell.execute_reply": "2021-07-22T07:32:15.482686Z",
          "shell.execute_reply.started": "2021-07-22T07:32:14.228653Z"
        },
        "id": "znGH9meZbTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import transformers\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import BertWordPieceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:54:32.391417Z",
          "iopub.status.busy": "2021-07-22T07:54:32.391075Z",
          "iopub.status.idle": "2021-07-22T07:54:33.257988Z",
          "shell.execute_reply": "2021-07-22T07:54:33.257144Z",
          "shell.execute_reply.started": "2021-07-22T07:54:32.391387Z"
        },
        "id": "aGokzdJ9bTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "\n",
        "def bert_encode(data, maximum_length):\n",
        "    input_ids=[]\n",
        "    attention_masks=[]\n",
        "    for text in data:\n",
        "        encoded = tokenizer.encode_plus(text, add_special_tokens=True, max_length = maximum_length, pad_to_max_length=True, return_attention_mask=True)\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids), np.array(attention_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:54:33.259685Z",
          "iopub.status.busy": "2021-07-22T07:54:33.259349Z",
          "iopub.status.idle": "2021-07-22T07:54:33.273524Z",
          "shell.execute_reply": "2021-07-22T07:54:33.272394Z",
          "shell.execute_reply.started": "2021-07-22T07:54:33.259650Z"
        },
        "id": "EU2JCaKIbTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "np.max(df['text'].apply(lambda x: len(x.split())).values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:54:33.276340Z",
          "iopub.status.busy": "2021-07-22T07:54:33.275614Z",
          "iopub.status.idle": "2021-07-22T07:54:35.551130Z",
          "shell.execute_reply": "2021-07-22T07:54:35.550271Z",
          "shell.execute_reply.started": "2021-07-22T07:54:33.276302Z"
        },
        "id": "5gVUFY2TbTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bt_text = df['text']\n",
        "bt_label = df['label_num']\n",
        "\n",
        "bt_ids, bt_masks = bert_encode(bt_text, 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:55:00.389165Z",
          "iopub.status.busy": "2021-07-22T07:55:00.388843Z",
          "iopub.status.idle": "2021-07-22T07:55:00.395860Z",
          "shell.execute_reply": "2021-07-22T07:55:00.394876Z",
          "shell.execute_reply.started": "2021-07-22T07:55:00.389134Z"
        },
        "id": "nv9Vcr15bTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertModel\n",
        "def create_model(bert_model):\n",
        "    \n",
        "    input_ids = tf.keras.Input(shape=(80,),dtype='int32')\n",
        "    attention_masks = tf.keras.Input(shape=(80,),dtype='int32')\n",
        "\n",
        "    output = bert_model([input_ids,attention_masks])\n",
        "    output = output[1]\n",
        "    output = tf.keras.layers.Dense(32,activation='relu')(output)\n",
        "    output = tf.keras.layers.Dropout(0.2)(output)\n",
        "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:55:00.839558Z",
          "iopub.status.busy": "2021-07-22T07:55:00.839248Z",
          "iopub.status.idle": "2021-07-22T07:55:02.088081Z",
          "shell.execute_reply": "2021-07-22T07:55:02.087204Z",
          "shell.execute_reply.started": "2021-07-22T07:55:00.839529Z"
        },
        "id": "lFBNx-OnbTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:55:02.089989Z",
          "iopub.status.busy": "2021-07-22T07:55:02.089653Z",
          "iopub.status.idle": "2021-07-22T07:55:03.710168Z",
          "shell.execute_reply": "2021-07-22T07:55:03.709421Z",
          "shell.execute_reply.started": "2021-07-22T07:55:02.089952Z"
        },
        "id": "oUQ0B9JDbTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = create_model(bert_model)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:55:04.647001Z",
          "iopub.status.busy": "2021-07-22T07:55:04.646683Z",
          "iopub.status.idle": "2021-07-22T07:58:31.201495Z",
          "shell.execute_reply": "2021-07-22T07:58:31.200677Z",
          "shell.execute_reply.started": "2021-07-22T07:55:04.646971Z"
        },
        "id": "2hr4E8LrbTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "history = model.fit([bt_ids, bt_masks], bt_label, validation_split=0.25, epochs=3, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T07:59:57.943497Z",
          "iopub.status.busy": "2021-07-22T07:59:57.943102Z",
          "iopub.status.idle": "2021-07-22T07:59:58.310835Z",
          "shell.execute_reply": "2021-07-22T07:59:58.310017Z",
          "shell.execute_reply.started": "2021-07-22T07:59:57.943461Z"
        },
        "id": "Jwi_PISCbTzu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "learning_curve(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNyb4y6sbTzv"
      },
      "source": [
        "<h2>Working with disaster tweets - Dataset</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:09:24.533079Z",
          "iopub.status.busy": "2021-07-22T08:09:24.532752Z",
          "iopub.status.idle": "2021-07-22T08:09:24.577964Z",
          "shell.execute_reply": "2021-07-22T08:09:24.577233Z",
          "shell.execute_reply.started": "2021-07-22T08:09:24.533048Z"
        },
        "id": "v3uw3v5bbTzv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
        "df_test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
        "\n",
        "df_train = df_train.dropna(axis=1)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:09:25.076135Z",
          "iopub.status.busy": "2021-07-22T08:09:25.075829Z",
          "iopub.status.idle": "2021-07-22T08:09:25.091756Z",
          "shell.execute_reply": "2021-07-22T08:09:25.090742Z",
          "shell.execute_reply.started": "2021-07-22T08:09:25.076099Z"
        },
        "id": "JJcC1IWgbTzv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.groupby('target').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:09:25.622436Z",
          "iopub.status.busy": "2021-07-22T08:09:25.622041Z",
          "iopub.status.idle": "2021-07-22T08:09:25.632282Z",
          "shell.execute_reply": "2021-07-22T08:09:25.631320Z",
          "shell.execute_reply.started": "2021-07-22T08:09:25.622401Z"
        },
        "id": "uc1p_iP7bTzv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# We can use .agg('count').values also\n",
        "class_counts = df.groupby('target').id.count().values\n",
        "class_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:09:26.243209Z",
          "iopub.status.busy": "2021-07-22T08:09:26.242758Z",
          "iopub.status.idle": "2021-07-22T08:09:26.271044Z",
          "shell.execute_reply": "2021-07-22T08:09:26.270321Z",
          "shell.execute_reply.started": "2021-07-22T08:09:26.243168Z"
        },
        "id": "SBrm21-6bTzv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "        x=['Fake disaster'],\n",
        "        y=[class_counts[0]],\n",
        "        name='Fake',\n",
        "        text=[class_counts[0]],\n",
        "        textposition='auto',\n",
        "        marker_color=pblue\n",
        ")\n",
        "             )\n",
        "fig.add_trace(go.Bar(\n",
        "        x=['Real disaster'],\n",
        "        y=[class_counts[1]],\n",
        "        name='Real',\n",
        "        text=[class_counts[1]],\n",
        "        textposition='auto',\n",
        "        marker_color=pg\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Class distribution in the dataset')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:11:11.062423Z",
          "iopub.status.busy": "2021-07-22T08:11:11.062048Z",
          "iopub.status.idle": "2021-07-22T08:11:11.086729Z",
          "shell.execute_reply": "2021-07-22T08:11:11.085981Z",
          "shell.execute_reply.started": "2021-07-22T08:11:11.062388Z"
        },
        "id": "r4e5184YbTzv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "len_real = df[df['target']==1].text.apply(lambda x: len(x.split())).value_counts().sort_index()\n",
        "len_fake = df[df['target']==0].text.apply(lambda x: len(x.split())).value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:11:11.819983Z",
          "iopub.status.busy": "2021-07-22T08:11:11.819681Z",
          "iopub.status.idle": "2021-07-22T08:11:11.834761Z",
          "shell.execute_reply": "2021-07-22T08:11:11.833757Z",
          "shell.execute_reply.started": "2021-07-22T08:11:11.819955Z"
        },
        "id": "4Kj8G-pvbTzv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=len_real.index,\n",
        "    y=len_real.values,\n",
        "    name='Real disaster',\n",
        "    fill='tozeroy',\n",
        "    marker_color=pblue,\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=len_fake.index,\n",
        "    y=len_fake.values,\n",
        "    name='Fake disaster',\n",
        "    fill='tozeroy',\n",
        "    marker_color=pg,\n",
        "))\n",
        "fig.update_layout(\n",
        "    title='<span style=\"font-size:32px; font-family:Times New Roman\">Data Roles in Different Fields</span>'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UasL5kOibTzv"
      },
      "source": [
        "<h2>Data pre-processing and cleaning</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:12:08.714962Z",
          "iopub.status.busy": "2021-07-22T08:12:08.714596Z",
          "iopub.status.idle": "2021-07-22T08:12:08.725597Z",
          "shell.execute_reply": "2021-07-22T08:12:08.724306Z",
          "shell.execute_reply.started": "2021-07-22T08:12:08.714928Z"
        },
        "id": "DIzgUwnebTzv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def remove_url(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    return re.sub(html, '', text)\n",
        "\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub(\n",
        "        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n",
        "        '', \n",
        "        text\n",
        "    )\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    \n",
        "    text = remove_url(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = remove_html(text)\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:12:24.244854Z",
          "iopub.status.busy": "2021-07-22T08:12:24.244518Z",
          "iopub.status.idle": "2021-07-22T08:12:24.251086Z",
          "shell.execute_reply": "2021-07-22T08:12:24.249861Z",
          "shell.execute_reply.started": "2021-07-22T08:12:24.244824Z"
        },
        "id": "0SCQj4C0bTzv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Testing the function\n",
        "remove_emoji(\"Omg another Earthquake \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:13:43.323292Z",
          "iopub.status.busy": "2021-07-22T08:13:43.322927Z",
          "iopub.status.idle": "2021-07-22T08:13:43.331652Z",
          "shell.execute_reply": "2021-07-22T08:13:43.330843Z",
          "shell.execute_reply.started": "2021-07-22T08:13:43.323257Z"
        },
        "id": "J8tD8IjLbTzv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "stopw = stopwords.words('english')\n",
        "more = ['u', 'im', 'c']\n",
        "stopw = stopw + more\n",
        "\n",
        "stemmer = nltk.SnowballStemmer('english')\n",
        "\n",
        "def data_cleaning(text):\n",
        "    text = clean_text(text)\n",
        "    text = ' '.join(stemmer.stem(word) for word in text.split(' ') if word not in stopw)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:14:26.512590Z",
          "iopub.status.busy": "2021-07-22T08:14:26.512265Z",
          "iopub.status.idle": "2021-07-22T08:14:29.156752Z",
          "shell.execute_reply": "2021-07-22T08:14:29.155884Z",
          "shell.execute_reply.started": "2021-07-22T08:14:26.512562Z"
        },
        "id": "BMvdTaUqbTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df['cleaned_text'] = df['text'].apply(data_cleaning)\n",
        "df_test['cleaned_text'] = df_test['text'].apply(data_cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:14:35.291289Z",
          "iopub.status.busy": "2021-07-22T08:14:35.290934Z",
          "iopub.status.idle": "2021-07-22T08:14:35.303483Z",
          "shell.execute_reply": "2021-07-22T08:14:35.302675Z",
          "shell.execute_reply.started": "2021-07-22T08:14:35.291258Z"
        },
        "id": "evHQRSjubTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YImcqD1ebTzw"
      },
      "source": [
        "<h2>WordCloud analysis</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:16:41.701747Z",
          "iopub.status.busy": "2021-07-22T08:16:41.701410Z",
          "iopub.status.idle": "2021-07-22T08:16:41.708942Z",
          "shell.execute_reply": "2021-07-22T08:16:41.708124Z",
          "shell.execute_reply.started": "2021-07-22T08:16:41.701717Z"
        },
        "id": "UlTNh6cibTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def corpus(df, label):\n",
        "    corpus=[]\n",
        "    for x in df[df['target']==label]['cleaned_text'].str.split():\n",
        "        for i in x:\n",
        "            corpus.append(i)\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:17:30.189612Z",
          "iopub.status.busy": "2021-07-22T08:17:30.189246Z",
          "iopub.status.idle": "2021-07-22T08:17:30.223007Z",
          "shell.execute_reply": "2021-07-22T08:17:30.222055Z",
          "shell.execute_reply.started": "2021-07-22T08:17:30.189580Z"
        },
        "id": "TAvzb2_WbTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "corpus_reald = corpus(df, 1)\n",
        "dic = defaultdict(int)\n",
        "\n",
        "# Creating a dictionary with frequency of words\n",
        "for word in corpus_reald:\n",
        "    dic[word]+=1\n",
        "    \n",
        "# Sorting words by descending frequency\n",
        "top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:18:14.554111Z",
          "iopub.status.busy": "2021-07-22T08:18:14.553780Z",
          "iopub.status.idle": "2021-07-22T08:18:15.491798Z",
          "shell.execute_reply": "2021-07-22T08:18:15.490977Z",
          "shell.execute_reply.started": "2021-07-22T08:18:14.554078Z"
        },
        "id": "aO1RqmmRbTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "twitter_mask = np.array(Image.open('/kaggle/input/masksforwordclouds/twitter_mask3.jpg'))\n",
        "\n",
        "wc = WordCloud(\n",
        "    background_color='white', \n",
        "    max_words=200, \n",
        "    mask=twitter_mask,\n",
        ")\n",
        "wc.generate(' '.join(text for text in df.loc[df['target'] == 1, 'cleaned_text']))\n",
        "plt.figure(figsize=(18,10))\n",
        "plt.title('Wordcloud for real disasters', \n",
        "          fontdict={'size': 22,  'verticalalignment': 'bottom'})\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:18:51.126053Z",
          "iopub.status.busy": "2021-07-22T08:18:51.125740Z",
          "iopub.status.idle": "2021-07-22T08:18:51.160411Z",
          "shell.execute_reply": "2021-07-22T08:18:51.159386Z",
          "shell.execute_reply.started": "2021-07-22T08:18:51.126022Z"
        },
        "id": "OM1U6i4ubTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "corpus_faked = corpus(df, 0)\n",
        "dic = defaultdict(int)\n",
        "\n",
        "# Creating a dictionary with frequency of words\n",
        "for word in corpus_faked:\n",
        "    dic[word]+=1\n",
        "    \n",
        "# Sorting words by descending frequency\n",
        "top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T08:19:01.673311Z",
          "iopub.status.busy": "2021-07-22T08:19:01.672958Z",
          "iopub.status.idle": "2021-07-22T08:19:02.702570Z",
          "shell.execute_reply": "2021-07-22T08:19:02.701583Z",
          "shell.execute_reply.started": "2021-07-22T08:19:01.673280Z"
        },
        "id": "B4xqFATubTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "twitter_mask = np.array(Image.open('/kaggle/input/masksforwordclouds/twitter_mask3.jpg'))\n",
        "\n",
        "wc = WordCloud(\n",
        "    background_color='white', \n",
        "    max_words=200, \n",
        "    mask=twitter_mask,\n",
        ")\n",
        "wc.generate(' '.join(text for text in df.loc[df['target'] == 0, 'cleaned_text']))\n",
        "plt.figure(figsize=(18,10))\n",
        "plt.title('Wordcloud for fake disasters', \n",
        "          fontdict={'size': 22,  'verticalalignment': 'bottom'})\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93oumaJObTzw"
      },
      "source": [
        "<h2>Model creation and testing</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:06:28.922720Z",
          "iopub.status.busy": "2021-07-22T09:06:28.922303Z",
          "iopub.status.idle": "2021-07-22T09:06:28.936338Z",
          "shell.execute_reply": "2021-07-22T09:06:28.935206Z",
          "shell.execute_reply.started": "2021-07-22T09:06:28.922682Z"
        },
        "id": "tG0vorribTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "x = df['cleaned_text']\n",
        "y = df['target']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:07:22.521309Z",
          "iopub.status.busy": "2021-07-22T09:07:22.520961Z",
          "iopub.status.idle": "2021-07-22T09:07:24.778392Z",
          "shell.execute_reply": "2021-07-22T09:07:24.777059Z",
          "shell.execute_reply.started": "2021-07-22T09:07:22.521278Z"
        },
        "id": "dkdZoJYVbTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "    ('bow', CountVectorizer()), \n",
        "    ('tfid', TfidfTransformer()),  \n",
        "    ('model', xgb.XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='auc',\n",
        "    ))\n",
        "])\n",
        "\n",
        "pipe.fit(x_train, y_train)\n",
        "yp_test = pipe.predict(x_test)\n",
        "yp_train = pipe.predict(x_train)\n",
        "\n",
        "print('Training accuracy: {}'.format(metrics.accuracy_score(y_train, yp_train)))\n",
        "print('Testing accuracy: {}'.format(metrics.accuracy_score(y_test, yp_test)))\n",
        "\n",
        "seaborn_conf(y_test, yp_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TercnODvbTzw"
      },
      "source": [
        "<h3>GloVE - LSTM</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:08:40.940821Z",
          "iopub.status.busy": "2021-07-22T09:08:40.940455Z",
          "iopub.status.idle": "2021-07-22T09:08:40.946303Z",
          "shell.execute_reply": "2021-07-22T09:08:40.944986Z",
          "shell.execute_reply.started": "2021-07-22T09:08:40.940788Z"
        },
        "id": "rh-1scsQbTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "training = df['cleaned_text'].values\n",
        "testing = df_test['cleaned_text'].values\n",
        "# Target labels\n",
        "labels = df['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:09:13.218136Z",
          "iopub.status.busy": "2021-07-22T09:09:13.217808Z",
          "iopub.status.idle": "2021-07-22T09:09:13.353145Z",
          "shell.execute_reply": "2021-07-22T09:09:13.352071Z",
          "shell.execute_reply.started": "2021-07-22T09:09:13.218098Z"
        },
        "id": "EC42lAPWbTzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Word tokenizer\n",
        "tk = Tokenizer()\n",
        "tk.fit_on_texts(training)\n",
        "\n",
        "vocab = len(tk.word_index)+1\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:11:06.545098Z",
          "iopub.status.busy": "2021-07-22T09:11:06.544779Z",
          "iopub.status.idle": "2021-07-22T09:11:06.550958Z",
          "shell.execute_reply": "2021-07-22T09:11:06.549711Z",
          "shell.execute_reply.started": "2021-07-22T09:11:06.545068Z"
        },
        "id": "Lf1gQo1mbTzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def metric_calculation(y_test, y_pred):\n",
        "    print(\"F1-score: \", f1_score(y_pred, y_test))\n",
        "    print(\"Precision: \", precision_score(y_pred, y_test))\n",
        "    print(\"Recall: \", recall_score(y_pred, y_test))\n",
        "    print(\"Acuracy: \", accuracy_score(y_pred, y_test))\n",
        "    print(\"-\"*50)\n",
        "    print(classification_report(y_pred, y_test))\n",
        "    \n",
        "def embeddings(corpus): \n",
        "    return tk.texts_to_sequences(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:11:07.372422Z",
          "iopub.status.busy": "2021-07-22T09:11:07.372048Z",
          "iopub.status.idle": "2021-07-22T09:11:07.385962Z",
          "shell.execute_reply": "2021-07-22T09:11:07.384997Z",
          "shell.execute_reply.started": "2021-07-22T09:11:07.372389Z"
        },
        "id": "2IutjFRjbTzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "len_train = np.max(df['cleaned_text'].apply(lambda x: len(x)))\n",
        "len_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:11:08.418325Z",
          "iopub.status.busy": "2021-07-22T09:11:08.417989Z",
          "iopub.status.idle": "2021-07-22T09:11:08.622115Z",
          "shell.execute_reply": "2021-07-22T09:11:08.621113Z",
          "shell.execute_reply.started": "2021-07-22T09:11:08.418293Z"
        },
        "id": "_bkBioLcbTzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_padded_sentences = pad_sequences(\n",
        "    embeddings(training), \n",
        "    len_train, \n",
        "    padding='post'\n",
        ")\n",
        "test_padded_sentences = pad_sequences(\n",
        "    embeddings(testing), \n",
        "    len_train,\n",
        "    padding='post'\n",
        ")\n",
        "\n",
        "train_padded_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:12:13.262079Z",
          "iopub.status.busy": "2021-07-22T09:12:13.261741Z",
          "iopub.status.idle": "2021-07-22T09:12:13.297256Z",
          "shell.execute_reply": "2021-07-22T09:12:13.296243Z",
          "shell.execute_reply.started": "2021-07-22T09:12:13.262030Z"
        },
        "id": "5SIJE5ZkbTzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# As we've already created a GloVe dictionary in the SMS-dataset, we will start with the matrix\n",
        "\n",
        "embedding_matrix = np.zeros((vocab, embedding_dim))\n",
        "\n",
        "for word, index in tk.word_index.items():\n",
        "    embedding_vector = embedding_dict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "        \n",
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:12:53.878915Z",
          "iopub.status.busy": "2021-07-22T09:12:53.878599Z",
          "iopub.status.idle": "2021-07-22T09:12:53.886133Z",
          "shell.execute_reply": "2021-07-22T09:12:53.885136Z",
          "shell.execute_reply.started": "2021-07-22T09:12:53.878887Z"
        },
        "id": "iRNmJYFDbTzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train_padded_sentences, labels, test_size=0.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:13:49.243248Z",
          "iopub.status.busy": "2021-07-22T09:13:49.242913Z",
          "iopub.status.idle": "2021-07-22T09:13:49.517684Z",
          "shell.execute_reply": "2021-07-22T09:13:49.516890Z",
          "shell.execute_reply.started": "2021-07-22T09:13:49.243201Z"
        },
        "id": "BcdIDlzXbTzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=embedding_matrix.shape[0], \n",
        "                   output_dim=embedding_matrix.shape[1],\n",
        "                   weights=[embedding_matrix],\n",
        "                   input_length=max_len\n",
        "                   )\n",
        "         )\n",
        "model.add(Bidirectional(LSTM(max_len, return_sequences=True, recurrent_dropout=0.15)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(max_len, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(max_len, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:14:06.394978Z",
          "iopub.status.busy": "2021-07-22T09:14:06.394663Z",
          "iopub.status.idle": "2021-07-22T09:31:58.564618Z",
          "shell.execute_reply": "2021-07-22T09:31:58.563697Z",
          "shell.execute_reply.started": "2021-07-22T09:14:06.394949Z"
        },
        "id": "pgBblxl8bTzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "checkpoint = ModelCheckpoint('model.h5', monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, verbose = 1, patience = 5,                        min_lr = 0.001)\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs = 7,batch_size = 32,validation_data = (x_test, y_test),verbose = 1,callbacks = [reduce_lr, checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-22T09:31:58.566858Z",
          "iopub.status.busy": "2021-07-22T09:31:58.566518Z",
          "iopub.status.idle": "2021-07-22T09:31:58.881684Z",
          "shell.execute_reply": "2021-07-22T09:31:58.880892Z",
          "shell.execute_reply.started": "2021-07-22T09:31:58.566819Z"
        },
        "id": "iNj_eU4lbTzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "learning_curve(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-22T09:32:01.848661Z",
          "iopub.status.idle": "2021-07-22T09:32:01.849464Z"
        },
        "id": "CeYSI2M0bTzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "preds = model.predict_classes(x_test)\n",
        "metric_calculation(preds, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0PVci8NbTzx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
